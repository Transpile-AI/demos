{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep_7G754_PUX"
      },
      "source": [
        "# Accelerating PyTorch models with JAX"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accelerate your Pytorch models by converting them to JAX for faster inference."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DEoCDYyRsBLu"
      },
      "source": [
        "⚠️ If you are running this notebook in Colab, you will have to install `Ivy` and some dependencies manually. You can do so by running the cell below ⬇️\n",
        "\n",
        "If you want to run the notebook locally but don't have Ivy installed just yet, you can check out the [Get Started section of the docs.](https://unify.ai/docs/ivy/overview/get_started.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnyOp6JusBLv"
      },
      "outputs": [],
      "source": [
        "!pip install -q ivy\n",
        "!pip install -q transformers\n",
        "!pip install -q dm-haiku\n",
        "exit()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1XHBgZOAsBLw"
      },
      "source": [
        "For the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on **\"Runtime > Restart Runtime\"**. Once the runtime has been restarted you should skip the previous cell 😄"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l39vDhft8F8X"
      },
      "source": [
        "Let's now import Ivy and the libraries we'll use in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29c5UttUsK17"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import ivy\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoFeatureExtractor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8DUmTxOz8Q25"
      },
      "source": [
        "Now we can load a ResNet model and its corresponding feature extractor from Hugging Face transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl2RJ_KlsNy2"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "arch_name = \"ResNet\"\n",
        "checkpoint_name = \"microsoft/resnet-50\"\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(checkpoint_name)\n",
        "model = AutoModel.from_pretrained(checkpoint_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BseySf2g82kx"
      },
      "source": [
        "We will also need a sample image to pass during tracing, so let's use the feature extractor to get the corresponding torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK_mu3brssdT"
      },
      "outputs": [],
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "inputs = feature_extractor(\n",
        "    images=image, return_tensors=\"pt\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z_4af3mZ88Wl"
      },
      "source": [
        "And finally, let's transpile the model to haiku!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEBx4fwFvmC-"
      },
      "outputs": [],
      "source": [
        "transpiled_graph = ivy.transpile(model, to=\"haiku\", kwargs=inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xe-vwz9v9Czh"
      },
      "source": [
        "After transpiling our model, we can see what's the improvement in runtime efficiency like. For this let's compile the original PyTorch model using `torch.compile`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mUKwqWnvx1Q"
      },
      "outputs": [],
      "source": [
        "inputs = feature_extractor(\n",
        "    images=image, return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "model.to(\"cuda\")\n",
        "\n",
        "def _f(**kwargs):\n",
        "  return model(**kwargs)\n",
        "\n",
        "comp_model = torch.compile(_f)\n",
        "_ = comp_model(**inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg1o9T-B9aIr"
      },
      "source": [
        "Let's now do the equivalent transformation in our new haiku model by using JAX just in time compilation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQk3gbihv483"
      },
      "outputs": [],
      "source": [
        "inputs_jax = feature_extractor(\n",
        "    images=image, return_tensors=\"jax\"\n",
        ")\n",
        "\n",
        "import haiku as hk\n",
        "\n",
        "def _forward(**kwargs):\n",
        "  module = transpiled_graph()\n",
        "  return module(**kwargs).last_hidden_state\n",
        "\n",
        "_forward = jax.jit(_forward)\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "jax_mlp_forward = hk.transform(_forward)\n",
        "params = jax_mlp_forward.init(rng=rng_key, **inputs_jax)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0ulQ5z1n9SuR"
      },
      "source": [
        "Now that we have both models optimized, let's see how their runtime speeds compare to each other!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LOd86nDv0uW"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "_ = comp_model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7r02dlwv6ce"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "out = jax_mlp_forward.apply(params, None, **inputs_jax)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uR2BAWZC-hvh"
      },
      "source": [
        "As expected, we have made the model significantly faster with just one line of code, getting a ~2x increase in its execution speed! 🚀"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, as a sanity check, let's load a different image and make sure that the results are the same in both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"http://images.cocodataset.org/train2017/000000283921.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "inputs = feature_extractor(\n",
        "    images=image, return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "inputs_jax = feature_extractor(\n",
        "    images=image, return_tensors=\"jax\"\n",
        ")\n",
        "out_torch = comp_model(**inputs)\n",
        "out_jax = jax_mlp_forward.apply(params, None, **inputs_jax)\n",
        "\n",
        "np.allclose(out_torch.last_hidden_state.detach().cpu().numpy(), out_jax, atol=1e-4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's pretty much it! The results from both models are the same, but we have achieved a solid speed up by using Ivy's transpiler to convert the model to JAX!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
