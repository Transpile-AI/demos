{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2I9lo9vMW5GB"
      },
      "source": [
        "# Accelerating MMPreTrain models with JAX"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OBvR4DRxW7TK"
      },
      "source": [
        "Accelerate your MMPreTrain models by converting them to JAX for faster inference."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhyVeXOXbeM"
      },
      "source": [
        "Install OpenMIM and mmpretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX7ZSGstXcP5",
        "outputId": "8bc4e80f-7366-4762-eac6-ea708a887125"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q openmim && mim install -q \"mmpretrain>=1.0.0rc8\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DEoCDYyRsBLu"
      },
      "source": [
        "⚠️ If you are running this notebook in Colab, you will have to install `Ivy` and some dependencies manually. You can do so by running the cell below ⬇️\n",
        "\n",
        "If you want to run the notebook locally but don't have Ivy installed just yet, you can check out the [Get Started section of the docs.](https://unify.ai/docs/ivy/overview/get_started.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnyOp6JusBLv",
        "outputId": "bc0f8b9d-d520-4b2f-c3b4-08c96ddc4db0"
      },
      "outputs": [],
      "source": [
        "!pip install ivy\n",
        "!pip install dm-haiku\n",
        "exit()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHvhhvd5y4U"
      },
      "source": [
        "For the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on **\"Runtime > Restart Runtime\"**. Once the runtime has been restarted you should skip the previous cell 😄"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y1GtTjJOdUgG"
      },
      "source": [
        "Let's now import Ivy and the libraries we'll use in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "29c5UttUsK17"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import ivy\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision\n",
        "from mmpretrain import get_model, list_models\n",
        "from mmengine import ConfigDict"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QN0oSCTwdkKg"
      },
      "source": [
        "Sanity check to make sure checkpoint name is correct against mmpretrain's [model zoo](https://mmpretrain.readthedocs.io/en/latest/modelzoo_statistics.html#pretrained-models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvIzkkovaLw",
        "outputId": "e1d0e775-26c7-4ef7-8219-ee09258dd191"
      },
      "outputs": [],
      "source": [
        "checkpoint_name = \"convnext-tiny_32xb128-noema_in1k\"\n",
        "list_models(checkpoint_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m6EF-otSdaxn"
      },
      "source": [
        "Now we can load the ConvNext model from OpenMMLab's mmpretrain library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl2RJ_KlsNy2",
        "outputId": "5edf2689-0d98-49a2-e9c9-67b69583e623"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "model = get_model(checkpoint_name, pretrained=True, device='cuda')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b9orwOx9eDhx"
      },
      "source": [
        "We will also need a sample image to pass during tracing, so let's use the appropriate transforms to get the corresponding torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_scale(cfg):\n",
        "    if type(cfg) == ConfigDict:\n",
        "        if cfg.get('type', False) and cfg.get('scale', False):\n",
        "            return cfg['scale']\n",
        "        else:\n",
        "            for k in cfg.keys():\n",
        "                input_shape = get_scale(cfg[k])\n",
        "                if input_shape:\n",
        "                    return input_shape\n",
        "    elif type(cfg) == list:\n",
        "        for block in cfg:\n",
        "            input_shape = get_scale(block)\n",
        "            if input_shape:\n",
        "                return input_shape\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sd_ywJE77Pwp"
      },
      "outputs": [],
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "input_shape = get_scale(model._config.train_pipeline)\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((input_shape, input_shape)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dapWhFdRegVG"
      },
      "source": [
        "And finally, let's transpile the model to haiku!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJGzJLmYuu-a",
        "outputId": "192b7211-ffb0-40f3-bea9-2f436bbc7d7d"
      },
      "outputs": [],
      "source": [
        "transpiled_graph = ivy.transpile(model, to=\"haiku\", args=(tensor_image,))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tqUkwEhEemfX"
      },
      "source": [
        "After transpiling our model, we can see what's the improvement in runtime efficiency like. For this let's compile the original PyTorch model using `torch.compile`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AZVq72BQ7lHV"
      },
      "outputs": [],
      "source": [
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "def _f(args):\n",
        "  return model(args)\n",
        "\n",
        "comp_model = torch.compile(_f)\n",
        "_ = comp_model(tensor_image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg1o9T-B9aIr"
      },
      "source": [
        "Let's now do the equivalent transformation in our new haiku model by using JAX just in time compilation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YQk3gbihv483"
      },
      "outputs": [],
      "source": [
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")\n",
        "np_image = tensor_image.detach().cpu().numpy()\n",
        "\n",
        "import haiku as hk\n",
        "\n",
        "def _forward(args):\n",
        "  module = transpiled_graph()\n",
        "  return module(args)\n",
        "\n",
        "_forward = jax.jit(_forward)\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "jax_mlp_forward = hk.transform(_forward)\n",
        "params = jax_mlp_forward.init(rng=rng_key, args=np_image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0ulQ5z1n9SuR"
      },
      "source": [
        "Now that we have both models optimized, let's see how their runtime speeds compare to each other!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LOd86nDv0uW",
        "outputId": "31f645bf-296c-411c-d662-286ff158e591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.46 ms ± 72.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "_ = comp_model(tensor_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7r02dlwv6ce",
        "outputId": "2116425e-59a7-45f3-dfd2-eee5e5efe8ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.79 ms ± 133 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "out = jax_mlp_forward.apply(params, None, np_image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uR2BAWZC-hvh"
      },
      "source": [
        "As expected, we have made the model significantly faster with just one line of code, getting a ~2x increase in its execution speed! 🚀"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nGu1iznHr8LI"
      },
      "source": [
        "Finally, as a sanity check, let's load a different image and make sure that the results are the same in both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6aMaMbbr8LI",
        "outputId": "cb64d185-88df-4cd4-96b3-b5d3baab3b77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = \"http://images.cocodataset.org/train2017/000000283921.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")\n",
        "np_image = tensor_image.detach().cpu().numpy()\n",
        "out_torch = comp_model(tensor_image)\n",
        "out_jax = jax_mlp_forward.apply(params, None, np_image)\n",
        "\n",
        "np.allclose(out_torch.detach().cpu().numpy(), out_jax, atol=1e-4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ChfnzP1rfdC4"
      },
      "source": [
        "That's pretty much it! The results from both models are the same, but we have achieved a solid speed up by using Ivy's transpiler to convert the model to JAX!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
