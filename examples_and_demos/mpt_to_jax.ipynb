{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating [MPT-1B](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b) with JAX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerate MosaicML's MPT-1B by converting them to JAX for faster inference.  \n",
    "Note: This notebook was tested on a CPU server with 112 GB of RAM. The minimum memory requirement is 64 GB, so it might not be runnable on Colab. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ If you are running this notebook in Colab, you will have to install `Ivy` and some dependencies manually. You can do so by running the cell below ⬇️\n",
    "\n",
    "If you want to run the notebook locally but don't have Ivy installed just yet, you can check out the [Get Started section of the docs.](https://unify.ai/docs/ivy/overview/get_started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ivy\n",
    "!pip install -q transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on **\"Runtime > Restart Runtime\"**. Once the runtime has been restarted you should skip the previous cell 😄"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import Ivy and the libraries we'll use in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import ivy\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the MPT model from Hugging Face transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  'mosaicml/mpt-1b-redpajama-200b',\n",
    "  trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a sample input to pass during tracing, so let's use the appropriate model methods to get the dummy tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = model.main_input_name\n",
    "expanded_dummy = model._expand_inputs_for_generation(expand_size=1, input_ids=model.dummy_inputs.get(id))\n",
    "model_input = {id: expanded_dummy[0]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's transpile the model to jax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpiled_graph = ivy.transpile(model.__call__, source=\"torch\", to=\"jax\", kwargs=model_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do JAX just in time compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_inputs = {}\n",
    "for key, value in model_input.items():\n",
    "        jit_inputs[key] = jax.numpy.array(value.cpu().numpy())\n",
    "\n",
    "def fn(x):\n",
    "    return transpiled_graph(**x).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted = jax.jit(fn)\n",
    "_ = jitted(jit_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both models, let's see how their runtime speeds compare to each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 4.25 ms, total: 1.97 s\n",
      "Wall time: 369 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = model(**model_input).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 29.9 ms, total: 1.69 s\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = jitted(jit_inputs).block_until_ready()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have made the model faster with just one line of code! 🚀  \n",
    "Note: While the above numbers are from a CPU server, it is recommended to try this out on GPU - which is where we see most of the latency speedups from transpiling. 😊"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as a sanity check, let's load a different image and make sure that the results are the same in both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = model.main_input_name\n",
    "expanded_dummy = model._expand_inputs_for_generation(expand_size=1, input_ids=model.dummy_inputs.get(id))\n",
    "model_input = {id: expanded_dummy[0]}\n",
    "\n",
    "out_torch = model(**model_input).logits\n",
    "out_jax = jitted(jit_inputs).block_until_ready()\n",
    "\n",
    "np.allclose(out_torch.detach().cpu().numpy(), out_jax, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
