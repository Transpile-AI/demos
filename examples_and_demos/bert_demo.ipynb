{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, we show how a [Bidirectional Encoder From Transformers (Bert)](https://pytorch.org/hub/huggingface_pytorch-transformers/) model written in Ivy native code  used for **Sequence Classification** and **MLM**, and integrated with all three of the major ML frameworks: **PyTorch**, **TensorFlow** and **JAX**."
      ],
      "metadata": {
        "id": "Em2lO-yK0qbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First of all**\n",
        "You first have to enable gpu support if you are in **Google Colab**\n",
        "\n",
        "Go to **Runtime** -> **Change runtime type** -> **Choose Gpu**"
      ],
      "metadata": {
        "id": "UebEZHTXwGkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the dependecies\n",
        "\n",
        "- ivy `ivy library`\n",
        "- ivy_models `ivy models library`\n",
        "- transformers ` Transformers library to get the pretrained model`\n",
        "\n",
        "**If you have all of this libraries installed you can save some time and skip this cell if not you should run this cell and restart the runtime**"
      ],
      "metadata": {
        "id": "SC8O9zdlqIx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ivy\n",
        "\n",
        "!pip install -q dm-haiku\n",
        "!git clone https://github.com/unifyai/models.git\n",
        "%cd models\n",
        "!python3 -m pip install -q -e .\n",
        "\n",
        "!python3 -m pip install transformers\n",
        "\n",
        "exit()"
      ],
      "metadata": {
        "id": "_NrE1GA21w9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the modules"
      ],
      "metadata": {
        "id": "iUf2i-Py7fvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import ivy\n",
        "import ivy_models\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\") # to ignore the warnings"
      ],
      "metadata": {
        "id": "RkeXJSDQ6q0r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n"
      ],
      "metadata": {
        "id": "rUpgv-NA8O0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  load the pretrained Model and tokenizer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPDb_AFj8ql6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_base = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_base = bert_base.eval() # for inference and evaluation\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "9s42i-ja8BKz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare some samples to test on\n",
        "\n",
        "texts = [\"i did't really like your tone.\"]\n",
        "inputs = bert_tokenizer(texts,\n",
        "                        padding='longest',\n",
        "                        return_tensors='pt',\n",
        "                        max_length=512)\n",
        "inputs"
      ],
      "metadata": {
        "id": "oJhhb3bqsLGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466da54a-e960-46f3-aedf-bfc6ef82ce46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2106, 1005, 1056, 2428, 2066, 2115, 4309, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We get the transformers Bert pooler outputs to compare it with ivy bert outputs**"
      ],
      "metadata": {
        "id": "dfKPlX7v8kwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch model inference\n",
        "with torch.no_grad():\n",
        "   bert_output = bert_base(**inputs).pooler_output"
      ],
      "metadata": {
        "id": "wYpUctVr8ijT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ivy model Inference with numpy"
      ],
      "metadata": {
        "id": "Gf6z4-Pw8yx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First We import [the native ivy code for Bert](https://github.com/unifyai/models/blob/master/ivy_models/bert/bert.py)**"
      ],
      "metadata": {
        "id": "UNzGh6qS6sJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ivy.set_backend('numpy')\n",
        "ivy_bert = ivy_models.bert_base_uncased(pretrained=True)"
      ],
      "metadata": {
        "id": "ZRd8Vnqf84lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ivy_inputs = {k:ivy.asarray(v.numpy()) for k, v in inputs.items()}\n",
        "ivy_bert.compile(kwargs=ivy_inputs)"
      ],
      "metadata": {
        "id": "lxNcbpmm-QGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ivy inference with Sequence Classification"
      ],
      "metadata": {
        "id": "I-7UV4a3D6GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ivy_output = ivy_bert(**ivy_inputs)['pooler_output']"
      ],
      "metadata": {
        "id": "rXdi3ljr-Zb2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"logits shapes, Ivy: {list(ivy_output.shape)}, torch: {list(bert_output.shape)}\")\n",
        "logits_close = np.allclose(ivy_output, bert_output.detach().numpy(),rtol=0.005,atol=0.005)\n",
        "if logits_close:\n",
        "  print(f\"logits are equal\")\n",
        "else:\n",
        "  print(f\"logits are not equal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAdWIRsg42r_",
        "outputId": "2e9ad1aa-b426-4d03-f44e-dea04af9a8a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shapes, Ivy: [1, 768], torch: [1, 768]\n",
            "logits are equal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ivy model inference with tensorflow"
      ],
      "metadata": {
        "id": "JJZKvyEWE1_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ivy.set_backend('tensorflow')\n",
        "ivy_bert = ivy_models.bert_base_uncased(pretrained=True)"
      ],
      "metadata": {
        "id": "BgLhnshLEGCG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's compare the runtime before and after compilation**"
      ],
      "metadata": {
        "id": "j234lUfR3rqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "st = time.time()\n",
        "ivy_inputs = {k:ivy.asarray(v.numpy()) for k, v in inputs.items()}\n",
        "ivy_output = ivy_bert(**ivy_inputs)['pooler_output']\n",
        "fn = time.time()\n",
        "print(f\"Finished in {(fn - st):.2f} secs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfwOKgTT3lNQ",
        "outputId": "7aa63a9a-fc2d-43c2-e471-4452809cdbef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished in 95.08 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"logits shapes, Ivy: {list(ivy_output.shape)}, torch: {list(bert_output.shape)}\")\n",
        "logits_close = np.allclose(ivy_output.numpy(), bert_output.detach().numpy(),rtol=0.005,atol=0.005)\n",
        "if logits_close:\n",
        "  print(f\"logits are equal\")\n",
        "else:\n",
        "  print(f\"logits are not equal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPInB4K64z7Y",
        "outputId": "b535e2e0-324c-47a8-81a4-4f0c98d85f80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shapes, Ivy: [1, 768], torch: [1, 768]\n",
            "logits are equal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we compile the model**\n",
        "\n",
        "We repeat the same procedure before"
      ],
      "metadata": {
        "id": "hE5MBcpu8zX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ivy_bert.compile(kwargs=ivy_inputs)"
      ],
      "metadata": {
        "id": "WhubB0HrEJBD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st = time.time()\n",
        "ivy_output = ivy_bert(**ivy_inputs)['pooler_output']\n",
        "fn = time.time()\n",
        "print(f\"Finished in {(fn - st):.2f} secs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JluVC5dr3itt",
        "outputId": "1d5228d5-84a7-4c8f-d513-df37ab69474b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished in 0.60 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see that the big difference in inference runtime before and after compilation**"
      ],
      "metadata": {
        "id": "XzGqZB0791br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"logits shapes, Ivy: {list(ivy_output.shape)}, torch: {list(bert_output.shape)}\")\n",
        "logits_close = np.allclose(ivy_output.numpy(), bert_output.detach().numpy(),rtol=0.005,atol=0.005)\n",
        "if logits_close:\n",
        "  print(f\"logits are equal\")\n",
        "else:\n",
        "  print(f\"logits are not equal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZrkaPMs0PgU",
        "outputId": "d0b8bf23-19e8-4fe6-ad0d-f1d7e37243e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shapes, Ivy: [1, 768], torch: [1, 768]\n",
            "logits are equal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ivy model inference with Jax"
      ],
      "metadata": {
        "id": "1II9BgCP-Ez-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jax.config.update('jax_enable_x64', True)\n",
        "ivy.set_backend(\"jax\")\n",
        "ivy_bert = ivy_models.bert_base_uncased(pretrained=True)"
      ],
      "metadata": {
        "id": "tMVQ3qpR0S2c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ivy_inputs = {k:ivy.asarray(v.numpy()) for k, v in inputs.items()}\n",
        "ivy_bert.compile(kwargs=ivy_inputs)"
      ],
      "metadata": {
        "id": "7JUJRf_6-ZoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ivy_output = ivy_bert(**ivy_inputs)['pooler_output']\n",
        "print(f\"logits shapes, Ivy: {list(ivy_output.shape)}, torch: {list(bert_output.shape)}\")\n",
        "ref = jnp.array( bert_output.detach())\n",
        "logits_close = jnp.allclose(ivy_output, ref,rtol=0.005,atol=0.005)\n",
        "if logits_close:\n",
        "  print(f\"logits are equal\")\n",
        "else:\n",
        "  print(f\"logits are not equal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6__J6K0_Xk0",
        "outputId": "b50c73f7-ca80-4ba7-b0fd-6ec60f680f69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shapes, Ivy: [1, 768], torch: [1, 768]\n",
            "logits are equal\n"
          ]
        }
      ]
    }
  ]
}