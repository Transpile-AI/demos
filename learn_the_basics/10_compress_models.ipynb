{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing Machine learning models (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ This feature is still work in proccess. The demo is a high level view on what to expect and how would we expect to use it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress any machine learning model with one line! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ If you are running this notebook in Colab, you will have to install `Ivy` and some dependencies manually. You can do so by running the cell below ⬇️\n",
    "\n",
    "If you want to run the notebook locally but don't have Ivy installed just yet, you can check out the [Get Started section of the docs.](https://unify.ai/docs/ivy/overview/get_started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ivy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model compressing is machine learning technique that helps you to deploy a model to edge device with constraints hardware. \n",
    "In this demo we assume that you have a model already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ivy\n",
    "\n",
    "model = ivy.model.load(\"path/to/model.pt\")\n",
    "compressed_model = ivy.compress(model, technique=\"pruning\", compression_ratio=0.5, error_margin=0.1, optimizer=\"size\")\n",
    "ivy.model.save(compressed_model, \"path/to/compressed_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can find from the code above the model compression function direct and we provided to it the arguments that we like to work. \n",
    "If you do not provide any arguments it will go with the defaults which is the ones we have written above. \n",
    "as for the arguments it's: \n",
    "- technique: the technigqu for compression which you can select one of the next four: pruning,Quantization, Knowledge Distillation, Low Matrix Factorization. \n",
    "- compression_ratio: it's a metric to work on optimising for it, yet this will be constrainst with the error margin.  \n",
    "- error_margin: error margin is the loss that we can accept during the model.\n",
    "- optimizer: if you only provide this, our algorithm will try to find the best optimiser without considering the first technique your have provided. then it will provide the best model based on your need. The options that you can provide is: size, speed, memeory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading this, today we learned how to use Ivy compress function to improve our model, and create a model that we can use for our edge computing! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
